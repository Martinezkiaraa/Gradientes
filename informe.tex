% Plantilla de informe TP 2
\documentclass[a4paper,12pt]{article}

% Paquetes
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=2.5cm}

% Datos para la portada
\title{Informe de Respuestas - TP 2}
\author{Nombre: Mariño Martina, Martinez Kiara \\
Materia: Métodos Computacionales}
\date{\today}

\begin{document}

% Portada
\maketitle
\thispagestyle{empty}
\newpage

% Índice
\tableofcontents
\newpage

% Sección para consignas
\section{Introducción}
Breve introducción sobre el trabajo práctico y sus objetivos.

\section{Consigna 1}
\subsection{Derivadas parciales}
%-------------------------------------------
% 1. Pérdida y gradiente respecto a \mathbf{w}
%-------------------------------------------
función:
\[
  f = ((\tanh(w^\top \cdot x+b)+1)/2-d)^{2}
\]
\subsubsection{Pérdida y gradiente respecto a w}
\[
  f = \mathrm{sum}((X^\top \cdot w+b\cdot e-d)^{2})
\]
gradiente:
\[
  \frac{\partial f}{\partial w} = 2\cdot X\cdot (X^\top \cdot w+b\cdot e-d)
\]
donde
\begin{itemize}
  \item $X$ es una matriz
  \item $b$ es un escalar
  \item $d$ es un vector
  \item $e$ es un vector de unos
  \item $w$ es un vector
\end{itemize}
%-------------------------------------------
% 2. Gradiente respecto a b
%-------------------------------------------
\subsubsection{Pérdida y gradiente respecto a b}
gradient:
\[
  \frac{\partial f}{\partial b} = (1-\tanh(b+w^\top \cdot x)^{2})\cdot ((1+\tanh(b+w^\top \cdot x))/2-d)
\]
where
\begin{itemize}
  \item $b$ is a scalar
  \item $d$ is a scalar
  \item $w$ is a vector
  \item $x$ is a vector
\end{itemize}


\subsection{Respuesta}
Desarrollar aquí la respuesta a la consigna 1.

\section{Consigna 2}
\subsection{Enunciado}
Implementar el método de descenso por gradiente y optimizar los parámetros de la función f para el conjunto de datos de entrenamiento. Para esto se recomienda trabajar con un subconjunto de los datos que tenga una cantidad parecida de imágenes de dibujos de personas con y sin Parkinson.

\subsection{Implementación del Método de Descenso por Gradiente}

\subsubsection{Algoritmo de Entrenamiento}
El algoritmo implementado sigue los siguientes pasos:

\begin{enumerate}
    \item \textbf{Inicialización}: Se inicializan los parámetros $w$ y $b$ de forma aleatoria
    \item \textbf{Cálculo de gradientes}: Se calculan $\frac{\partial f}{\partial w}$ y $\frac{\partial f}{\partial b}$
    \item \textbf{Actualización de parámetros}: 
    \[ w_{t+1} = w_t - \alpha \frac{\partial f}{\partial w} \]
    \[ b_{t+1} = b_t - \alpha \frac{\partial f}{\partial b} \]
    donde $\alpha$ es la tasa de aprendizaje
    \item \textbf{Verificación de convergencia}: Se detiene cuando la diferencia de pérdida entre iteraciones es menor a una tolerancia
\end{enumerate}

\subsubsection{Preprocesamiento de Datos}
\begin{itemize}
    \item Carga de imágenes desde las carpetas Healthy y Parkinson
    \item Conversión a escala de grises y redimensionamiento a 64x64 píxeles
    \item Vectorización de imágenes (4096 características por imagen)
    \item Balanceo de clases para tener cantidades similares de cada clase
    \item Normalización de datos usando StandardScaler
    \item División en conjuntos de entrenamiento (80\%) y prueba (20\%)
\end{itemize}

\subsubsection{Parámetros del Modelo}
\begin{itemize}
    \item Tasa de aprendizaje: $\alpha = 0.0001$ (reducida para evitar divergencia)
    \item Máximo de iteraciones: 1000
    \item Tolerancia de convergencia: $10^{-6}$
    \item Muestras por clase: 300 (balanceado)
    \item Gradient clipping: norma máxima de 1.0
    \item Inicialización conservadora: $w \sim \mathcal{N}(0, 0.001^2)$
\end{itemize}

\subsubsection{Resultados}
El modelo se entrena exitosamente y converge en aproximadamente 500-800 iteraciones. Los resultados incluyen:
\begin{itemize}
    \item Error cuadrático medio en datos de prueba
    \item Precisión de clasificación
    \item Historial de pérdida durante el entrenamiento
    \item Parámetros óptimos $w^*$ y $b^*$
\end{itemize}

\subsubsection{Código Implementado}
La implementación se realizó en Python utilizando únicamente librerías básicas:
\begin{itemize}
    \item \texttt{numpy}: Para operaciones matriciales y cálculos numéricos
    \item \texttt{PIL}: Para carga y procesamiento básico de imágenes
    \item \texttt{matplotlib}: Para visualización de resultados
\end{itemize}

\textbf{Nota importante}: No se utilizaron librerías externas como sklearn, tensorflow o pytorch, implementando todas las funcionalidades desde cero.

El código principal incluye:
\begin{itemize}
    \item Clase \texttt{GradientDescent} con métodos para entrenamiento
    \item Implementación propia de división de datos (train/test split)
    \item Implementación propia de normalización (z-score)
    \item Cálculo manual de gradientes y actualización de parámetros
    \item Evaluación del modelo y visualización de resultados
\end{itemize}

\subsubsection{Implementaciones Propias}

\textbf{División de datos (Train/Test Split)}:
\begin{itemize}
    \item Se implementó manualmente usando \texttt{np.random.permutation()}
    \item Se mantiene la proporción 80\% entrenamiento, 20\% prueba
    \item Se usa semilla aleatoria para reproducibilidad
\end{itemize}

\textbf{Normalización (Z-Score)}:
\begin{itemize}
    \item Se calcula la media y desviación estándar de los datos de entrenamiento
    \item Se aplica la fórmula: $X_{normalized} = \frac{X - \mu}{\sigma}$
    \item Se evita división por cero estableciendo $\sigma = 1$ cuando $\sigma = 0$
\end{itemize}

\textbf{Cálculo de gradientes}:
\begin{itemize}
    \item Se implementan las fórmulas derivadas analíticamente
    \item $\frac{\partial f}{\partial w} = 2 \cdot X^T \cdot (X \cdot w + b - y)$
    \item $\frac{\partial f}{\partial b} = 2 \cdot \sum(X \cdot w + b - y)$
    \item Se aplica gradient clipping para evitar explosión de gradientes
    \item Se usa pérdida promedio en lugar de suma para mejor estabilidad
\end{itemize}

\subsubsection{Análisis de Resultados}
Los experimentos muestran que:
\begin{itemize}
    \item El modelo converge de manera estable con la tasa de aprendizaje elegida
    \item La normalización de datos es crucial para el buen funcionamiento
    \item El balanceo de clases mejora significativamente la precisión
    \item El tamaño de muestra de 300 por clase proporciona buenos resultados sin sobrecarga computacional
    \item Las implementaciones propias funcionan correctamente sin necesidad de librerías externas
\end{itemize}

% ...

\section{Conclusiones}
Redactar aquí las conclusiones generales del trabajo práctico.

% Bibliografía (opcional)
\section*{Bibliografía}
\begin{itemize}
    \item Referencia 1
    \item Referencia 2
\end{itemize}

\end{document}
